# -*- coding: utf-8 -*-
"""SLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14Ndw3_0oGXSroAaW58xtB0v_Asve5FWI

**
1.Download the book.

2.Clean the text.

3.Split it into chunks.

4.Print the total number of chunks and the first chunk as an example.**
"""

import re
import requests

def download_book(url):
    """Download a book from a URL efficiently."""
    response = requests.get(url, stream=True)
    response.raise_for_status()
    return response.text

def clean_text(text):
    """Remove special characters, digits, and Project Gutenberg headers/footers while keeping formatting intact."""
    start_marker = "*** START OF THE PROJECT GUTENBERG EBOOK"
    end_marker = "*** END OF THE PROJECT GUTENBERG EBOOK"

    start_index = text.find(start_marker)
    end_index = text.find(end_marker)

    if start_index != -1 and end_index != -1:
        text = text[start_index + len(start_marker):end_index]

    # Remove unwanted sections (e.g., Table of Contents, repeated chapter titles)
    text = re.sub(r'(?i)^\s*Contents.*?$|^\s*CHAPTER\s+[IVXLCDM]+\..*$', '', text, flags=re.MULTILINE)

    # Keep only readable characters while preserving punctuation
    text = re.sub(r'[^a-zA-Z0-9\s.,!?;:\'\"()-]', '', text)

    # Normalize spacing
    return re.sub(r'\s+', ' ', text).strip()

def split_into_chunks(text, chunk_size=500):
    """Split text into smaller chunks efficiently."""
    words = text.split()
    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]

# Example usage
if __name__ == "__main__":
    book_url = "https://www.gutenberg.org/cache/epub/11/pg11.txt"
    book_text = download_book(book_url)
    cleaned_text = clean_text(book_text)
    chunks = split_into_chunks(cleaned_text)

    print(f"Total chunks: {len(chunks)}")
    print("First chunk:", chunks[0])

!pip install faiss-cpu

import re
import requests
import faiss
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer

# Download & Process Book (from Module 1)
def download_book(url):
    response = requests.get(url)
    response.raise_for_status()
    return response.text

def clean_text(text):
    start_marker = "*** START OF THE PROJECT GUTENBERG EBOOK"
    end_marker = "*** END OF THE PROJECT GUTENBERG EBOOK"
    start_index = text.find(start_marker) + len(start_marker)
    end_index = text.find(end_marker)
    text = text[start_index:end_index]
    text = re.sub(r'[^a-zA-Z\s.,!?]', '', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def split_into_chunks(text, chunk_size=500):
    words = text.split()
    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]

# Compute Dense Embeddings using SentenceTransformers
def compute_dense_embeddings(chunks, model_name="all-MiniLM-L6-v2"):
    model = SentenceTransformer(model_name)
    embeddings = model.encode(chunks, convert_to_numpy=True)
    return model, embeddings

# Store Embeddings in FAISS for Fast Retrieval
def store_in_faiss(embeddings, chunks, index_file="faiss_index.pkl"):
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)

    with open(index_file, "wb") as f:
        pickle.dump({"index": index, "chunks": chunks}, f)

# üîπ Example Usage
if __name__ == "__main__":
    book_url = "https://www.gutenberg.org/cache/epub/11/pg11.txt"
    book_text = download_book(book_url)
    cleaned_text = clean_text(book_text)
    chunks = split_into_chunks(cleaned_text)

    model, embeddings = compute_dense_embeddings(chunks)

    store_in_faiss(embeddings, chunks)

    print(f"‚úÖ Dense Embeddings Computed & Stored in FAISS! Total Chunks: {len(chunks)}")

import faiss
import pickle
import numpy as np
from sentence_transformers import SentenceTransformer

# Load FAISS index and stored data
def load_faiss_index(filename="faiss_index.pkl"): # Changed filename to faiss_index.pkl
    with open(filename, "rb") as f: # Using filename variable to open the file.
        data = pickle.load(f)
    index = data['index'] # Accessing index from the loaded data dictionary
    return index, data

# Search FAISS for top-k relevant chunks
def retrieve_relevant_chunks(query, index, data, model, top_k=3):
    query_embedding = model.encode([query])
    _, indices = index.search(np.array(query_embedding, dtype=np.float32), top_k)

    retrieved_chunks = [data["chunks"][idx] for idx in indices[0]]
    return retrieved_chunks

# Example usage
if __name__ == "__main__":
    index, data = load_faiss_index()
    model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

    query = "What is the Cheshire Cat's advice?"
    relevant_chunks = retrieve_relevant_chunks(query, index, data, model)

    print("üîç Top Retrieved Chunks:")
    for i, chunk in enumerate(relevant_chunks, 1):
        print(f"\nChunk {i}: {chunk}")

import torch
import torch.nn as nn
import torch.optim as optim
from transformers import AutoModel, AutoTokenizer
from torch.utils.data import Dataset, DataLoader

# Load pre-trained model and tokenizer
MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
base_model = AutoModel.from_pretrained(MODEL_NAME)

# Define Custom Dataset
class QADataset(Dataset):
    def __init__(self, questions, answers, tokenizer, max_length=512):
        self.questions = questions
        self.answers = answers
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.questions)

    def __getitem__(self, idx):
        encoded = self.tokenizer(
            self.questions[idx], self.answers[idx],
            padding='max_length', truncation=True,
            max_length=self.max_length, return_tensors='pt'
        )
        return {key: val.squeeze(0) for key, val in encoded.items()}

# Dummy data (Replace with real QA pairs from the book)
questions = ["Who is the Cheshire Cat?", "What is the Queen's famous phrase?"]
answers = ["A grinning cat that Alice meets in Wonderland.", "Off with their heads!"]

# Prepare dataset and dataloader
dataset = QADataset(questions, answers, tokenizer)
dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

# Define Fine-Tuning Model
class FineTunedModel(nn.Module):
    def __init__(self, base_model):
        super(FineTunedModel, self).__init__()
        self.encoder = base_model
        self.fc = nn.Linear(self.encoder.config.hidden_size, 1)

    def forward(self, input_ids, attention_mask):
        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.last_hidden_state[:, 0, :]
        return self.fc(pooled_output)

model = FineTunedModel(base_model)
optimizer = optim.AdamW(model.parameters(), lr=5e-5)
criterion = nn.MSELoss()

# Fine-Tuning Loop
def train(model, dataloader, optimizer, criterion, epochs=3):
    model.train()
    for epoch in range(epochs):
        for batch in dataloader:
            optimizer.zero_grad()
            outputs = model(batch['input_ids'], batch['attention_mask'])
            loss = criterion(outputs.squeeze(), torch.ones_like(outputs.squeeze()))
            loss.backward()
            optimizer.step()
        print(f"Epoch {epoch+1} completed, Loss: {loss.item():.4f}")

# Train the model
train(model, dataloader, optimizer, criterion)

import torch
import torch.nn as nn
import torch.optim as optim
from transformers import AutoModel, AutoTokenizer
from torch.utils.data import Dataset, DataLoader
import faiss
import numpy as np

# Load pre-trained model and tokenizer
MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
base_model = AutoModel.from_pretrained(MODEL_NAME)

# Define Custom Dataset
class QADataset(Dataset):
    def __init__(self, questions, answers, tokenizer, max_length=512):
        self.questions = questions
        self.answers = answers
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.questions)

    def __getitem__(self, idx):
        encoded = self.tokenizer(
            self.questions[idx], self.answers[idx],
            padding='max_length', truncation=True,
            max_length=self.max_length, return_tensors='pt'
        )
        return {key: val.squeeze(0) for key, val in encoded.items()}

# Dummy data (Replace with real QA pairs from the book)
questions = ["Who is the Cheshire Cat?", "What is the Queen's famous phrase?"]
answers = ["A grinning cat that Alice meets in Wonderland.", "Off with their heads!"]

# Prepare dataset and dataloader
dataset = QADataset(questions, answers, tokenizer)
dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

# Define Fine-Tuning Model
class FineTunedModel(nn.Module):
    def __init__(self, base_model):
        super(FineTunedModel, self).__init__()
        self.encoder = base_model
        self.fc = nn.Linear(self.encoder.config.hidden_size, 1)

    def forward(self, input_ids, attention_mask, token_type_ids=None):
        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.last_hidden_state[:, 0, :]
        return self.fc(pooled_output)

model = FineTunedModel(base_model)
optimizer = optim.AdamW(model.parameters(), lr=5e-5)
criterion = nn.MSELoss()

# Fine-Tuning Loop
def train(model, dataloader, optimizer, criterion, epochs=3):
    model.train()
    for epoch in range(epochs):
        for batch in dataloader:
            optimizer.zero_grad()
            outputs = model(batch['input_ids'], batch['attention_mask'])
            loss = criterion(outputs.squeeze(), torch.ones_like(outputs.squeeze()))
            loss.backward()
            optimizer.step()
        print(f"Epoch {epoch+1} completed, Loss: {loss.item():.4f}")

# Train the model
train(model, dataloader, optimizer, criterion)

# FAISS Integration
class FAISSRetriever:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.index = faiss.IndexFlatL2(base_model.config.hidden_size)
        self.text_chunks = []

    def encode_text(self, text):
        encoded = self.tokenizer(text, padding=True, truncation=True, return_tensors="pt")
        with torch.no_grad():
            output = self.model.encoder(input_ids=encoded['input_ids'], attention_mask=encoded['attention_mask'])
            embedding = output.last_hidden_state[:, 0, :].numpy()
        return embedding

    def add_documents(self, docs):
        embeddings = np.vstack([self.encode_text(doc) for doc in docs])
        self.index.add(embeddings)
        self.text_chunks.extend(docs)

    def retrieve(self, query, top_k=2):
        query_embedding = self.encode_text(query)
        distances, indices = self.index.search(query_embedding, top_k)
        return [self.text_chunks[idx] for idx in indices[0]]

# Initialize retriever
retriever = FAISSRetriever(model, tokenizer)
retriever.add_documents(["Alice met the Cheshire Cat in Wonderland.", "The Queen often says 'Off with their heads!'"])

# Retrieval-Augmented Generation
class RAGModel(nn.Module):
    def __init__(self, base_model, retriever):
        super(RAGModel, self).__init__()
        self.base_model = base_model
        self.retriever = retriever

    def forward(self, query):
        retrieved_docs = self.retriever.retrieve(query)
        context = " ".join(retrieved_docs)
        input_text = query + " " + context
        encoded_input = tokenizer(input_text, return_tensors="pt")
        with torch.no_grad():
            # Get the hidden states from the encoder within base_model
            output = self.base_model.encoder(**encoded_input)
        return output.last_hidden_state[:, 0, :]

rag_model = RAGModel(model, retriever)

# Test RAG Model
query = "Who is the Cheshire Cat?"
retrieved_output = rag_model(query)
print(f"Retrieved answer embedding: {retrieved_output.shape}")

import torch
import torch.nn.functional as F
import numpy as np

# Function to evaluate the model using a test dataset
def evaluate_model(model, dataloader):
    model.eval()
    losses = []

    with torch.no_grad():
        for batch in dataloader:
            outputs = model(batch['input_ids'], batch['attention_mask'])
            loss = F.mse_loss(outputs.squeeze(), torch.ones_like(outputs.squeeze()))
            losses.append(loss.item())

    avg_loss = np.mean(losses)
    print(f"Evaluation Loss: {avg_loss:.4f}")

# Function to save the trained model
def save_model(model, path="fine_tuned_model.pth"):
    torch.save(model.state_dict(), path)
    print(f"Model saved to {path}")

def load_model(model, path="fine_tuned_model.pth"):
    model.load_state_dict(torch.load(path, weights_only=True))
    model.eval()
    print("Model loaded successfully.")
    return model


# Interactive function to ask questions
def ask_question(model, tokenizer, retriever, query):
    model.eval()

    retrieved_docs = retriever.retrieve(query)  # Get relevant context
    context = " ".join(retrieved_docs)
    input_text = query + " " + context

    encoded_input = tokenizer(input_text, return_tensors="pt")
    with torch.no_grad():
        output = model.encoder(input_ids=encoded_input['input_ids'], attention_mask=encoded_input['attention_mask'])
        answer_embedding = output.last_hidden_state[:, 0, :]

    print(f"Retrieved Answer Embedding: {answer_embedding.shape}")

# Example usage
evaluate_model(model, dataloader)  # Evaluate the model
save_model(model)  # Save the model

# Load and test the model
model = load_model(model)

# Ask a question
query = "Who is the Cheshire Cat?"
ask_question(model, tokenizer, retriever, query)

import torch
import torch.nn.functional as F
import numpy as np

def evaluate_model(model, dataloader):
    model.eval()
    losses = []

    with torch.no_grad():
        for batch in dataloader:
            outputs = model(batch['input_ids'], batch['attention_mask'])  # Get model output
            loss = F.mse_loss(outputs.squeeze(), torch.ones_like(outputs.squeeze()))  # Calculate MSE loss
            losses.append(loss.item())

    avg_loss = np.mean(losses)
    print(f"Evaluation Loss: {avg_loss:.4f}")
    return losses

# Function to save the trained model
def save_model(model, path="fine_tuned_model.pth"):
    torch.save(model.state_dict(), path)
    print(f"Model saved to {path}")

def load_model(model, path="fine_tuned_model.pth"):
    model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))
    model.eval()
    print("Model loaded successfully.")
    return model

def ask_question(model, tokenizer, retriever, query, max_length=100):
    model.eval()

    retrieved_docs = retriever.retrieve(query)  # Get relevant context
    context = " ".join(retrieved_docs) if retrieved_docs else "No relevant context found."

    input_text = f"Question: {query} Context: {context}"
    encoded_input = tokenizer(input_text, return_tensors="pt")

    with torch.no_grad():
        outputs = model(**encoded_input)

    print(f"üü¢ Question: {query}")
    print(f"üîµ Context: {context}")
    print(f"üü£ Model Output: {outputs}")  # Debugging output

    relevance_score = outputs.squeeze().item()
    print(f"üî¥ Relevance Score: {relevance_score}\n{'-'*50}")

# Example usage
evaluate_model(model, dataloader)  # Evaluate the model
save_model(model)  # Save the model

model = load_model(model)  # Load trained model

# Interactive Question Loop
while True:
    query = input("Enter your question (or type 'exit' to quit): ").strip()
    if query.lower() == "exit":
        print("Exiting program.")
        break
    ask_question(model, tokenizer, retriever, query)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
import torch
import numpy as np

# Function to plot evaluation loss
def plot_loss_curve(losses):
    plt.figure(figsize=(8, 5))
    plt.plot(losses, marker='o', linestyle='-', color='b', label='Loss per batch')
    plt.xlabel('Batch Number')
    plt.ylabel('Loss')
    plt.title('Model Loss During Evaluation')
    plt.legend()
    plt.grid()
    plt.show()


# Example usage
losses = [0.8, 0.6, 0.4, 0.35, 0.3]  # Dummy loss values, replace with actual loss data
plot_loss_curve(losses)

import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA

# Function to evaluate the model using a test dataset
def evaluate_model(model, dataloader):
    model.eval()
    losses = []

    with torch.no_grad():
        for batch in dataloader:
            outputs = model(batch['input_ids'], batch['attention_mask'])
            loss = F.mse_loss(outputs.squeeze(), torch.ones_like(outputs.squeeze()))
            losses.append(loss.item())

    avg_loss = np.mean(losses)
    print(f"Evaluation Loss: {avg_loss:.4f}")
    return losses  # Return losses for visualization

# Function to save the trained model
def save_model(model, path="fine_tuned_model.pth"):
    torch.save(model.state_dict(), path)
    print(f"Model saved to {path}")

# Function to load the trained model
def load_model(model, path="fine_tuned_model.pth"):
    model.load_state_dict(torch.load(path, weights_only=True))
    model.eval()
    print("Model loaded successfully.")
    return model

# Interactive function to ask questions
def ask_question(model, tokenizer, retriever, query):
    model.eval()

    retrieved_docs = retriever.retrieve(query)  # Get relevant context
    context = " ".join(retrieved_docs)
    input_text = query + " " + context

    encoded_input = tokenizer(input_text, return_tensors="pt")
    with torch.no_grad():
        output = model.encoder(input_ids=encoded_input['input_ids'], attention_mask=encoded_input['attention_mask'])
        answer_embedding = output.last_hidden_state[:, 0, :]

    print(f"Retrieved Answer Embedding: {answer_embedding.shape}")

# Function to plot evaluation loss
def plot_loss_curve(losses):
    plt.figure(figsize=(8, 5))
    plt.plot(losses, marker='o', linestyle='-', color='b', label='Loss per batch')
    plt.xlabel('Batch Number')
    plt.ylabel('Loss')
    plt.title('Model Loss During Evaluation')
    plt.legend()
    plt.grid()
    plt.show()

# Function to visualize answer embeddings using PCA
def visualize_embeddings(model, tokenizer, retriever, queries):
    model.eval()
    embeddings = []
    labels = []

    with torch.no_grad():
        for query in queries:
            retrieved_docs = retriever.retrieve(query)
            context = " ".join(retrieved_docs)
            input_text = query + " " + context

            encoded_input = tokenizer(input_text, return_tensors="pt")
            output = model.encoder(input_ids=encoded_input['input_ids'], attention_mask=encoded_input['attention_mask'])
            answer_embedding = output.last_hidden_state[:, 0, :].squeeze().cpu().numpy()

            embeddings.append(answer_embedding)
            labels.append(query)

    embeddings = np.array(embeddings)

    # Reduce dimensionality for visualization
    pca = PCA(n_components=2)
    reduced_embeddings = pca.fit_transform(embeddings)

    # Plot the embeddings
    plt.figure(figsize=(8, 5))
    sns.scatterplot(x=reduced_embeddings[:, 0], y=reduced_embeddings[:, 1])

    # Annotate points with query text
    for i, label in enumerate(labels):
        plt.annotate(label, (reduced_embeddings[i, 0], reduced_embeddings[i, 1]))

    plt.xlabel("PCA Component 1")
    plt.ylabel("PCA Component 2")
    plt.title("Visualization of Answer Embeddings")
    plt.grid()
    plt.show()

# Example usage
losses = evaluate_model(model, dataloader)  # Evaluate the model and get losses
plot_loss_curve(losses)  # Plot loss curve
save_model(model)  # Save the model

# Load and test the model
model = load_model(model)

# Ask a question
query = "Who is the Cheshire Cat?"
ask_question(model, tokenizer, retriever, query)

# Example queries for embedding visualization
queries = ["Who is Alice?", "What is Wonderland?", "Describe the Mad Hatter."]
visualize_embeddings(model, tokenizer, retriever, queries)